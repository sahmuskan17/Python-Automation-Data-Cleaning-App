{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0Yu30WtSzsI74W4IvdeBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahmuskan17/Python-Automation-Data-Cleaning-App/blob/main/Data_Cleaning_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python Automation Project--Data Cleaning App"
      ],
      "metadata": {
        "id": "wFqCHnzpwIKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**--Please create a python application that can take datasets and clean the data\n",
        "\n",
        "\n",
        "*   It should ask for datasets path and name\n",
        "*   It should check number of duplicates and remove all the duplicates\n",
        "*   It should keep a copy of all the duplicates\n",
        "*   It should check for missing values\n",
        "*   If any column that is numeric it should replace nulls with mean else it should drop that rows at the esnd it should save the data as clean data and also return duplicates records,clean data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "susDzYerwPdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import openpyxl\n",
        "import os\n",
        "import xlrd\n",
        "import random"
      ],
      "metadata": {
        "id": "KkVtZZ512SHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_path='/content/day16_walmart.xlsx'\n",
        "# data_name='walrmart_sales'\n",
        "\n",
        "def data_cleaning_master(data_path,data_name):\n",
        "\n",
        "  print(\"Thank you for giving the details!\")\n",
        "  sec=random.randint(1,4) #generating random numbers\n",
        "\n",
        "  #print delay message\n",
        "  print(f\"please wait for {sec} seconds! Checking file path\")\n",
        "  time.sleep(sec)\n",
        "\n",
        "  #checking if the file exists\n",
        "  if not os.path.exists(data_path):\n",
        "    print(\"Please enter the correct path!\")\n",
        "    return\n",
        "  else:\n",
        "    # check the file type\n",
        "    if data_path.endswith(\".csv\"):\n",
        "      print(\"This is a csv data\")\n",
        "      data=pd.read_csv(data_path, encoding_errors='ignore')\n",
        "\n",
        "    elif data_path.endswith(\".xlsx\"):\n",
        "      print(\"This is an excel data\")\n",
        "      data=pd.read_excel(data_path)\n",
        "\n",
        "    else:\n",
        "      print(\"Unknow file type\")\n",
        "      return\n",
        "\n",
        "  #showing number of records\n",
        "  print(f\"Dataset contains total rows {data.shape[0]}\\n Total columns:{data.shape[1]}\")\n",
        "\n",
        "  # Data cleaning\n",
        "  duplicates= data.duplicated()\n",
        "  total_duplicate= data.duplicated().sum()\n",
        "\n",
        "  print(f\"Total number of duplicates {total_duplicate}\")\n",
        "\n",
        "  #saving the duplicates\n",
        "  if total_duplicate>0:\n",
        "    duplicate_records =data[duplicates]\n",
        "    duplicate_records.to_csv(f'{data_name}_duplicates.csv',index=None)\n",
        "\n",
        "  #deleting the duplicates\n",
        "  df= data.drop_duplicates()\n",
        "\n",
        "  #Find missing values\n",
        "  total_missing_values= df.isnull().sum().sum()\n",
        "  missing_value_columns= df.isnull().sum()\n",
        "  print(f\"Total number of missing values {total_missing_values}\")\n",
        "  print(f\"Total number of missing values per column {missing_value_columns}\")\n",
        "\n",
        "  #dealing with missing values\n",
        "  #fillna--int,float\n",
        "  #dropna--object\n",
        "\n",
        "  columns=df.columns\n",
        "\n",
        "  for col in columns:\n",
        "    if df[col].dtype in (float,int):\n",
        "      df[col]= df[col].fillna(df[col].mean()) #filling with mean\n",
        "    else:\n",
        "      df.dropna(subset=col,inplace=True) #dropping the columns with null values\n",
        "\n",
        "  print(f\"Congrats! Dataset is cleaned! Number of rows: {df.shape[0]} Number of columns {df.shape[1]}\")\n",
        "  df.to_csv(f'{data_name}_clean.csv',index=None)\n",
        "  print(\"Dataset is saved!\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  print(\"Welcome to Data Cleaning Master!\")\n",
        "  #ask path and file name\n",
        "  data_path=input(\"Please enter dataset path:\")\n",
        "  data_name=input(\"Please enter dataset name:\")\n",
        "  data_cleaning_master(data_path,data_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1DbDyEr1aqN",
        "outputId": "4cd853e3-0fd0-453e-96c5-e763ff215712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Data Cleaning Master!\n",
            "Please enter dataset path:/content/day16_walmart.xlsx\n",
            "Please enter dataset name:'sales_data'\n",
            "Thank you for giving the details!\n",
            "please wait for 4 seconds! Checking file path\n",
            "This is an excel data\n",
            "Dataset contains total rows 19\n",
            " Total columns:12\n",
            "Total number of duplicates 0\n",
            "Total number of missing values 0\n",
            "Total number of missing values per column invoice_id        0\n",
            "branch            0\n",
            "city              0\n",
            "customer_type     0\n",
            "age               0\n",
            "gender            0\n",
            "product_line      0\n",
            "unit_price        0\n",
            "total             0\n",
            "date              0\n",
            "payment_method    0\n",
            "rating            0\n",
            "dtype: int64\n",
            "Congrats! Dataset is cleaned! Number of rows: 19 Number of columns 12\n",
            "Dataset is saved!\n"
          ]
        }
      ]
    }
  ]
}